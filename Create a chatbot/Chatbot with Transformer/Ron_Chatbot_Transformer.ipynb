{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeLifRZ80X_3"
      },
      "source": [
        "## Importing libraires"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2UCkxxiGLLV",
        "outputId": "bd058947-a8a7-4a9c-dcff-ffd2778ef711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhyYKk5uWykc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83a283c0-27a8-43cc-e41a-2c2ae29f6c9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "import re\n",
        "import gc\n",
        "import glob\n",
        "import json\n",
        "import pickle\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "from google.colab import drive\n",
        "from difflib import get_close_matches\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ine5s9qEWyCF",
        "outputId": "6f1a4146-5484-4f57-fbab-dd894f9c8dec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_directory = '/content/drive/My Drive/chatbot_data'"
      ],
      "metadata": {
        "id": "QQmmW5odEPMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnBz4v_J0dSI"
      },
      "source": [
        "## Find Parquet files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUOq9h-z0SYP",
        "outputId": "45db3145-2401-41e1-bc8d-165402f46736"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6 files: ['/content/drive/My Drive/chatbot_data/dataset/daily_dialog.parquet', '/content/drive/My Drive/chatbot_data/dataset/dataset0.parquet', '/content/drive/My Drive/chatbot_data/dataset/dataset1.parquet', '/content/drive/My Drive/chatbot_data/dataset/dataset2.parquet', '/content/drive/My Drive/chatbot_data/dataset/dataset3.parquet', '/content/drive/My Drive/chatbot_data/dataset/dataset4.parquet']\n",
            "Sample data from first file (first 2 rows):\n",
            "                                                text\n",
            "0  Say , Jim , how about going for a few beers af...\n",
            "1   You know that is tempting but is really not g...\n",
            "Number of rows in first file: 87170\n"
          ]
        }
      ],
      "source": [
        "# Find Parquet files\n",
        "def get_parquet_files(directory):\n",
        "    file_list = []\n",
        "    pattern = re.compile(r'dataset\\d+\\.parquet$|daily_dialog\\.parquet$')\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if pattern.match(file):\n",
        "                full_path = os.path.join(root, file)\n",
        "                file_list.append(full_path)\n",
        "\n",
        "    # Sort files: daily_dialog.parquet first, then dataset0.parquet, dataset1.parquet, etc.\n",
        "    def sort_key(x):\n",
        "        if 'daily_dialog' in x.lower():\n",
        "            return -1  # Place daily_dialog first\n",
        "        match = re.search(r'dataset(\\d+)\\.parquet', x, re.IGNORECASE)\n",
        "        return int(match.group(1)) if match else float('inf')  # Sort by number, or place at end if no match\n",
        "\n",
        "    file_list.sort(key=sort_key)\n",
        "    return file_list\n",
        "\n",
        "# Find and process the actual Parquet files\n",
        "parquet_files = get_parquet_files(f'{target_directory}/dataset/')\n",
        "print(f\"Found {len(parquet_files)} files:\", parquet_files)\n",
        "\n",
        "# Verify one file by loading a few rows\n",
        "df_sample = pd.read_parquet(parquet_files[0])\n",
        "print(\"Sample data from first file (first 2 rows):\")\n",
        "print(df_sample.head(2))\n",
        "print(\"Number of rows in first file:\", len(df_sample))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load DailyDialog dataset"
      ],
      "metadata": {
        "id": "IpbPtDNFGXU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load DailyDialog dataset\n",
        "dataset = load_dataset('daily_dialog', split='train', trust_remote_code=True)\n",
        "dialogs = dataset['dialog']  # List of dialogues (each dialogue is a list of utterances)\n",
        "\n",
        "# Convert to a DataFrame with a 'text' column (to match Cosmopedia format)\n",
        "texts = []\n",
        "for dialog in dialogs:\n",
        "    for utterance in dialog:\n",
        "        texts.append(utterance)\n",
        "\n",
        "# Create a DataFrame with just the 'text' column\n",
        "dialog_df = pd.DataFrame({'text': texts})\n",
        "\n",
        "# Save as a Parquet file in your Google Drive\n",
        "dialog_parquet_path = f'{target_directory}/dataset/daily_dialog.parquet'\n",
        "dialog_df.to_parquet(dialog_parquet_path)\n",
        "\n",
        "# Update parquet_files to include the new file\n",
        "parquet_files = get_parquet_files(f'{target_directory}/dataset/')\n",
        "print(f\"Updated parquet files: {parquet_files}\")\n",
        "\n",
        "# Verify the DailyDialog file\n",
        "df_dialog_sample = pd.read_parquet(dialog_parquet_path)\n",
        "print(\"Sample data from DailyDialog file (first 2 rows):\")\n",
        "print(df_dialog_sample.head(2))\n",
        "print(\"Number of rows in DailyDialog file:\", len(df_dialog_sample))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b36MsO8lGaQd",
        "outputId": "c62c8784-7740-42a0-8896-711dacecf2dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated parquet files: ['/content/drive/My Drive/chatbot_data/dataset/daily_dialog.parquet', '/content/drive/My Drive/chatbot_data/dataset/dataset0.parquet', '/content/drive/My Drive/chatbot_data/dataset/dataset1.parquet', '/content/drive/My Drive/chatbot_data/dataset/dataset2.parquet', '/content/drive/My Drive/chatbot_data/dataset/dataset3.parquet', '/content/drive/My Drive/chatbot_data/dataset/dataset4.parquet']\n",
            "Sample data from DailyDialog file (first 2 rows):\n",
            "                                                text\n",
            "0  Say , Jim , how about going for a few beers af...\n",
            "1   You know that is tempting but is really not g...\n",
            "Number of rows in DailyDialog file: 87170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU8To6Yn0zrd"
      },
      "source": [
        "## Build Vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save and load vocab functions"
      ],
      "metadata": {
        "id": "YnXrQ1wBXxbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_vocab(vocab, word_to_idx, idx_to_word, path=f'{target_directory}/config/'):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "    with open(os.path.join(path, 'vocab.json'), 'w') as f:\n",
        "        json.dump(vocab, f)\n",
        "\n",
        "    with open(os.path.join(path, 'word_to_idx.json'), 'w') as f:\n",
        "        json.dump(word_to_idx, f)\n",
        "\n",
        "    with open(os.path.join(path, 'idx_to_word.json'), 'w') as f:\n",
        "        json.dump(idx_to_word, f)\n",
        "\n",
        "    print(f\"✅ Vocabulary saved to {path}\")\n",
        "\n",
        "def load_vocab(path=f'{target_directory}/config/'):\n",
        "    with open(os.path.join(path, 'vocab.json'), 'r') as f:\n",
        "        vocab = json.load(f)\n",
        "\n",
        "    with open(os.path.join(path, 'word_to_idx.json'), 'r') as f:\n",
        "        word_to_idx = json.load(f)\n",
        "\n",
        "    with open(os.path.join(path, 'idx_to_word.json'), 'r') as f:\n",
        "        idx_to_word = json.load(f)\n",
        "\n",
        "    Config.vocab_size = len(vocab)\n",
        "    print(f\"✅ Vocabulary loaded from {path} (size: {Config.vocab_size})\")\n",
        "    return vocab, word_to_idx, idx_to_word"
      ],
      "metadata": {
        "id": "rkdl1SAaI-gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save and load config"
      ],
      "metadata": {
        "id": "kPFPFNZ2YDMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_config(path=f'{target_directory}/config/'):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    config_dict = {\n",
        "        'seq_length': Config.seq_length,\n",
        "        'batch_size': Config.batch_size,\n",
        "        'learning_rate': Config.learning_rate,\n",
        "        'device': Config.device,\n",
        "        'vocab_size': Config.vocab_size,\n",
        "        'pad_token': Config.pad_token,\n",
        "        'unk_token': Config.unk_token\n",
        "    }\n",
        "    with open(os.path.join(path, 'config.json'), 'w') as f:\n",
        "        json.dump(config_dict, f)\n",
        "    print(f\"✅ Config saved to {path}\")\n",
        "\n",
        "def load_config(path=f'{target_directory}/config/'):\n",
        "    with open(os.path.join(path, 'config.json'), 'r') as f:\n",
        "        config_dict = json.load(f)\n",
        "    Config.seq_length = config_dict['seq_length']\n",
        "    Config.batch_size = config_dict['batch_size']\n",
        "    Config.learning_rate = config_dict['learning_rate']\n",
        "    Config.device = config_dict['device']\n",
        "    Config.vocab_size = config_dict['vocab_size']\n",
        "    Config.pad_token = config_dict['pad_token']\n",
        "    Config.unk_token = config_dict['unk_token']\n",
        "    print(f\"✅ Config loaded from {path}\")"
      ],
      "metadata": {
        "id": "7aSJSDUaYE9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Vocab"
      ],
      "metadata": {
        "id": "kmAHvK9EZWP1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFhT-jh90xxl",
        "outputId": "e96aa653-187d-4431-efef-9aa6a02e3d3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 48293\n",
            "Sample vocab words: ['<pad>', '<unk>', 'swings', 'Aesthetic', 'Knicks', 'Development', 'cyclical', 'corresponds', 'drawback', 'avail']\n",
            "✅ Vocabulary saved to /content/drive/My Drive/chatbot_data/config/\n"
          ]
        }
      ],
      "source": [
        "# Config\n",
        "class Config:\n",
        "    tokenizer = word_tokenize\n",
        "    seq_length = 128\n",
        "    batch_size = 16\n",
        "    learning_rate = 0.001\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    vocab_size = None\n",
        "    pad_token = '<pad>'\n",
        "    unk_token = '<unk>'\n",
        "\n",
        "# Build vocabulary\n",
        "def build_vocab(parquet_files, min_freq=100):\n",
        "    word_counts = Counter()\n",
        "    texts = []\n",
        "    for file in parquet_files:\n",
        "        df = pd.read_parquet(file)\n",
        "        texts.extend(df['text'].astype(str).tolist())\n",
        "        del df\n",
        "        gc.collect()\n",
        "    for text in texts:\n",
        "        word_counts.update(Config.tokenizer(text))\n",
        "\n",
        "    vocab = [Config.pad_token, Config.unk_token] + list({\n",
        "        word for word, count in word_counts.items() if count >= min_freq and word.isalpha()\n",
        "    })\n",
        "    word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "    idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
        "    Config.vocab_size = len(vocab)\n",
        "    return vocab, word_to_idx, idx_to_word\n",
        "\n",
        "# Build the vocabulary and print results\n",
        "vocab, word_to_idx, idx_to_word = build_vocab(parquet_files, min_freq=100)\n",
        "print(f\"Vocabulary size: {Config.vocab_size}\")\n",
        "print(\"Sample vocab words:\", vocab[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save config and vocab"
      ],
      "metadata": {
        "id": "LVvXA5sJYNjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_vocab(vocab, word_to_idx, idx_to_word)\n",
        "save_config()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ep87FmSjYQBT",
        "outputId": "4acc57e4-0181-4f40-e758-20e1b06e1c82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Vocabulary saved to /content/drive/My Drive/chatbot_data/config/\n",
            "✅ Config saved to /content/drive/My Drive/chatbot_data/config/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2rM60Dc12jV"
      },
      "source": [
        "## Create dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYqA6vvP110e",
        "outputId": "e9095523-4f19-47d4-81ae-d91873b8e5f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Config loaded from /content/drive/My Drive/chatbot_data/config/\n",
            "✅ Vocabulary loaded from /content/drive/My Drive/chatbot_data/config/ (size: 48293)\n",
            "Sample input IDs: [38657, 1, 1, 1, 32783, 838, 8901, 40401, 29945, 46638]\n",
            "Sample labels: [1, 1, 1, 32783, 838, 8901, 40401, 29945, 46638, 1700]\n",
            "Dataset size: 87170\n"
          ]
        }
      ],
      "source": [
        "# Config\n",
        "class Config:\n",
        "    tokenizer = word_tokenize\n",
        "    seq_length = None\n",
        "    batch_size = None\n",
        "    learning_rate = None\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    vocab_size = None\n",
        "    pad_token = None\n",
        "    unk_token = None\n",
        "\n",
        "load_config()\n",
        "vocab, word_to_idx, idx_to_word = load_vocab()\n",
        "\n",
        "# Dataset\n",
        "class ChatDataset(Dataset):\n",
        "    def __init__(self, file_path, word_to_idx):\n",
        "        self.df = pd.read_parquet(file_path)\n",
        "        self.texts = self.df['text'].astype(str).tolist()\n",
        "        self.word_to_idx = word_to_idx\n",
        "        del self.df\n",
        "        gc.collect()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = [token.lower() for token in Config.tokenizer(self.texts[idx])]\n",
        "        input_ids = [self.word_to_idx.get(token, self.word_to_idx[Config.unk_token]) for token in tokens]\n",
        "        input_ids = input_ids[:Config.seq_length-1]\n",
        "        input_ids += [self.word_to_idx[Config.pad_token]] * (Config.seq_length-1 - len(input_ids))\n",
        "        labels = input_ids[1:] + [self.word_to_idx[Config.pad_token]]\n",
        "        return {\n",
        "            'input_ids': torch.tensor(input_ids).long(),\n",
        "            'labels': torch.tensor(labels).long()\n",
        "        }\n",
        "\n",
        "# Test the dataset with the first file\n",
        "dataset = ChatDataset(parquet_files[0], word_to_idx)\n",
        "sample = dataset[0]\n",
        "print(\"Sample input IDs:\", sample['input_ids'][:10].tolist())\n",
        "print(\"Sample labels:\", sample['labels'][:10].tolist())\n",
        "print(\"Dataset size:\", len(dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf6zjW4X7znO"
      },
      "source": [
        "## Tranformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5bNMJ5K71Ti",
        "outputId": "87cff3c2-6b8d-4516-9871-8593f4e7fe7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model initialized with vocab size: 48293\n",
            "Model parameters: 27964581\n",
            "Sample output shape: torch.Size([1, 127, 48293])\n"
          ]
        }
      ],
      "source": [
        "# Transformer Model\n",
        "class MiniTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, n_heads=4, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, Config.seq_length, d_model))\n",
        "        self.transformer = nn.TransformerDecoder(\n",
        "            nn.TransformerDecoderLayer(d_model, n_heads), num_layers\n",
        "        )\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        x = self.embed(input_ids) + self.pos_embed[:, :input_ids.size(1), :]\n",
        "        x = self.transformer(x, x)  # Self-attention\n",
        "        return self.fc(x)\n",
        "\n",
        "# Initialize model\n",
        "model = MiniTransformer(Config.vocab_size).to(Config.device)\n",
        "print(\"Model initialized with vocab size:\", Config.vocab_size)\n",
        "print(\"Model parameters:\", sum(p.numel() for p in model.parameters()))\n",
        "\n",
        "# Test forward pass with a sample\n",
        "sample_input = dataset[0]['input_ids'].unsqueeze(0).to(Config.device)  # Add batch dimension\n",
        "with torch.no_grad():\n",
        "    output = model(sample_input)\n",
        "print(\"Sample output shape:\", output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wjJpmRe8TDK"
      },
      "source": [
        "## Train transformer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function with checkpoint resumption\n",
        "def train_model(file_list, num_epochs=5, checkpoint_dir=f'{target_directory}/checkpoints/'):\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    model = MiniTransformer(Config.vocab_size).to(Config.device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=Config.learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=word_to_idx[Config.pad_token])\n",
        "\n",
        "    # Check for existing checkpoints\n",
        "    checkpoint_files = glob.glob(os.path.join(checkpoint_dir, 'epoch_*.pt'))\n",
        "    start_epoch = 1\n",
        "    if checkpoint_files:\n",
        "        # Find the latest checkpoint by epoch number\n",
        "        latest_checkpoint = max(checkpoint_files, key=lambda x: int(re.search(r'epoch_(\\d+)\\.pt', x).group(1)))\n",
        "        checkpoint = torch.load(latest_checkpoint, map_location=Config.device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        print(f\"Resuming from checkpoint: {latest_checkpoint} (starting at epoch {start_epoch})\")\n",
        "\n",
        "    # Train from start_epoch to num_epochs\n",
        "    for epoch in range(start_epoch, num_epochs + 1):\n",
        "        print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
        "        for file_idx, file_path in enumerate(file_list):\n",
        "            print(f\"Processing file {file_idx+1}/{len(file_list)}: {os.path.basename(file_path)}\")\n",
        "            dataset = ChatDataset(file_path, word_to_idx)\n",
        "            dataloader = DataLoader(dataset, batch_size=Config.batch_size, shuffle=True)\n",
        "\n",
        "            for batch_idx, batch in enumerate(dataloader):\n",
        "                inputs = batch['input_ids'].to(Config.device)\n",
        "                labels = batch['labels'].to(Config.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs.view(-1, Config.vocab_size), labels.view(-1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                if batch_idx % 50 == 0:\n",
        "                    print(f\"Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "            del dataset, dataloader\n",
        "            gc.collect()\n",
        "\n",
        "        # Save checkpoint\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f\"epoch_{epoch}.pt\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict()\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train on Cosmopedia files (exclude daily_dialog.parquet for now)\n",
        "cosmopedia_files = [f for f in parquet_files if 'daily_dialog' not in f]\n",
        "print(f\"Training on Cosmopedia files: {cosmopedia_files}\")\n",
        "\n",
        "model = train_model(cosmopedia_files, num_epochs=5)\n",
        "\n",
        "def save_trained_model(model, save_dir=f'{target_directory}/model_output/'):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    model_path = os.path.join(save_dir, 'chatbot_model.pt')\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"✅ Final trained model saved to {model_path}\")\n",
        "\n",
        "save_trained_model(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0qlZo6eVhyI",
        "outputId": "6aa9cab5-4b7b-433d-f397-ad2b4eb4e823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on Cosmopedia files: ['/content/drive/My Drive/chatbot_data/dataset/dataset0.parquet', '/content/drive/My Drive/chatbot_data/dataset/dataset1.parquet', '/content/drive/My Drive/chatbot_data/dataset/dataset2.parquet', '/content/drive/My Drive/chatbot_data/dataset/dataset3.parquet', '/content/drive/My Drive/chatbot_data/dataset/dataset4.parquet']\n",
            "Resuming from checkpoint: /content/drive/My Drive/chatbot_data/checkpoints/epoch_5.pt (starting at epoch 6)\n",
            "✅ Final trained model saved to /content/drive/My Drive/chatbot_data/model_output/chatbot_model.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = f'{target_directory}/checkpoints/'\n",
        "checkpoint_files = glob.glob(os.path.join(checkpoint_dir, 'epoch_*.pt'))\n",
        "latest_checkpoint = max(checkpoint_files, key=lambda x: int(re.search(r'epoch_(\\d+)\\.pt', x).group(1)))\n",
        "model = MiniTransformer(Config.vocab_size).to(Config.device)\n",
        "checkpoint = torch.load(latest_checkpoint, map_location=Config.device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "print(f\"✅ Loaded checkpoint: {latest_checkpoint}\")\n",
        "\n",
        "# Fine-tuning function\n",
        "def fine_tune_model(file_path, num_epochs=1, checkpoint_dir=f'{target_directory}/checkpoints/'):\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=Config.learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=word_to_idx[Config.pad_token])\n",
        "\n",
        "    dataset = ChatDataset(file_path, word_to_idx)\n",
        "    dataloader = DataLoader(dataset, batch_size=Config.batch_size, shuffle=True)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print(f\"\\nFine-tuning Epoch {epoch}/{num_epochs}\")\n",
        "        for batch_idx, batch in enumerate(dataloader):\n",
        "            inputs = batch['input_ids'].to(Config.device)\n",
        "            labels = batch['labels'].to(Config.device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.view(-1, Config.vocab_size), labels.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch_idx % 50 == 0:\n",
        "                print(f\"Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        # Save checkpoint\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f\"finetune_epoch_{epoch}.pt\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict()\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Fine-tuning checkpoint saved to {checkpoint_path}\")\n",
        "\n",
        "    del dataset, dataloader\n",
        "    gc.collect()\n",
        "    return model\n",
        "\n",
        "# Fine-tune on daily_dialog.parquet\n",
        "daily_dialog_file = [f for f in parquet_files if 'daily_dialog' in f][0]\n",
        "print(f\"Fine-tuning on: {daily_dialog_file}\")\n",
        "model = fine_tune_model(daily_dialog_file, num_epochs=1)\n",
        "\n",
        "# Save fine-tuned model\n",
        "def save_finetuned_model(model, save_dir=f'{target_directory}/model_output/'):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    model_path = os.path.join(save_dir, 'chatbot_finetuned.pt')\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"✅ Fine-tuned model weights saved to {model_path}\")\n",
        "\n",
        "save_finetuned_model(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "305r7LkBesNh",
        "outputId": "504447f9-ee1b-428e-f487-3e7f628d814e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded checkpoint: /content/drive/My Drive/chatbot_data/checkpoints/epoch_5.pt\n",
            "Fine-tuning on: /content/drive/My Drive/chatbot_data/dataset/daily_dialog.parquet\n",
            "\n",
            "Fine-tuning Epoch 1/1\n",
            "Batch 0, Loss: 14.1991\n",
            "Batch 50, Loss: 6.0999\n",
            "Batch 100, Loss: 6.0511\n",
            "Batch 150, Loss: 5.9441\n",
            "Batch 200, Loss: 5.7168\n",
            "Batch 250, Loss: 5.7921\n",
            "Batch 300, Loss: 5.9514\n",
            "Batch 350, Loss: 5.3910\n",
            "Batch 400, Loss: 5.2049\n",
            "Batch 450, Loss: 5.2378\n",
            "Batch 500, Loss: 5.5569\n",
            "Batch 550, Loss: 5.0712\n",
            "Batch 600, Loss: 5.4929\n",
            "Batch 650, Loss: 5.4384\n",
            "Batch 700, Loss: 5.1645\n",
            "Batch 750, Loss: 5.1523\n",
            "Batch 800, Loss: 5.5741\n",
            "Batch 850, Loss: 5.2248\n",
            "Batch 900, Loss: 5.0548\n",
            "Batch 950, Loss: 4.8425\n",
            "Batch 1000, Loss: 4.5918\n",
            "Batch 1050, Loss: 4.5528\n",
            "Batch 1100, Loss: 4.9230\n",
            "Batch 1150, Loss: 5.3549\n",
            "Batch 1200, Loss: 4.8522\n",
            "Batch 1250, Loss: 5.0114\n",
            "Batch 1300, Loss: 4.9219\n",
            "Batch 1350, Loss: 5.0446\n",
            "Batch 1400, Loss: 4.6027\n",
            "Batch 1450, Loss: 5.1551\n",
            "Batch 1500, Loss: 4.6188\n",
            "Batch 1550, Loss: 4.6313\n",
            "Batch 1600, Loss: 5.0622\n",
            "Batch 1650, Loss: 5.1358\n",
            "Batch 1700, Loss: 5.0388\n",
            "Batch 1750, Loss: 4.5415\n",
            "Batch 1800, Loss: 4.5627\n",
            "Batch 1850, Loss: 5.3492\n",
            "Batch 1900, Loss: 4.7657\n",
            "Batch 1950, Loss: 4.9204\n",
            "Batch 2000, Loss: 4.7614\n",
            "Batch 2050, Loss: 4.6053\n",
            "Batch 2100, Loss: 4.9159\n",
            "Batch 2150, Loss: 4.5051\n",
            "Batch 2200, Loss: 4.5155\n",
            "Batch 2250, Loss: 5.2964\n",
            "Batch 2300, Loss: 4.1895\n",
            "Batch 2350, Loss: 4.6341\n",
            "Batch 2400, Loss: 4.0345\n",
            "Batch 2450, Loss: 4.6638\n",
            "Batch 2500, Loss: 4.4163\n",
            "Batch 2550, Loss: 4.4959\n",
            "Batch 2600, Loss: 4.3811\n",
            "Batch 2650, Loss: 4.2423\n",
            "Batch 2700, Loss: 4.3676\n",
            "Batch 2750, Loss: 4.5044\n",
            "Batch 2800, Loss: 5.0617\n",
            "Batch 2850, Loss: 4.7622\n",
            "Batch 2900, Loss: 4.4539\n",
            "Batch 2950, Loss: 4.6691\n",
            "Batch 3000, Loss: 4.8557\n",
            "Batch 3050, Loss: 4.6558\n",
            "Batch 3100, Loss: 4.2302\n",
            "Batch 3150, Loss: 4.3684\n",
            "Batch 3200, Loss: 4.4215\n",
            "Batch 3250, Loss: 4.0695\n",
            "Batch 3300, Loss: 4.9768\n",
            "Batch 3350, Loss: 4.5328\n",
            "Batch 3400, Loss: 4.6668\n",
            "Batch 3450, Loss: 4.4745\n",
            "Batch 3500, Loss: 4.5411\n",
            "Batch 3550, Loss: 4.3753\n",
            "Batch 3600, Loss: 4.5531\n",
            "Batch 3650, Loss: 4.5700\n",
            "Batch 3700, Loss: 4.7231\n",
            "Batch 3750, Loss: 4.4265\n",
            "Batch 3800, Loss: 4.4684\n",
            "Batch 3850, Loss: 4.7815\n",
            "Batch 3900, Loss: 4.6609\n",
            "Batch 3950, Loss: 4.8411\n",
            "Batch 4000, Loss: 4.9154\n",
            "Batch 4050, Loss: 4.6898\n",
            "Batch 4100, Loss: 4.5397\n",
            "Batch 4150, Loss: 4.1654\n",
            "Batch 4200, Loss: 5.0748\n",
            "Batch 4250, Loss: 4.9793\n",
            "Batch 4300, Loss: 4.8933\n",
            "Batch 4350, Loss: 5.1419\n",
            "Batch 4400, Loss: 4.9912\n",
            "Batch 4450, Loss: 4.9880\n",
            "Batch 4500, Loss: 4.8379\n",
            "Batch 4550, Loss: 4.6608\n",
            "Batch 4600, Loss: 5.3439\n",
            "Batch 4650, Loss: 5.3550\n",
            "Batch 4700, Loss: 5.3248\n",
            "Batch 4750, Loss: 5.1684\n",
            "Batch 4800, Loss: 5.1386\n",
            "Batch 4850, Loss: 4.8369\n",
            "Batch 4900, Loss: 5.1118\n",
            "Batch 4950, Loss: 5.2216\n",
            "Batch 5000, Loss: 5.2883\n",
            "Batch 5050, Loss: 5.2698\n",
            "Batch 5100, Loss: 5.2102\n",
            "Batch 5150, Loss: 5.3072\n",
            "Batch 5200, Loss: 5.1378\n",
            "Batch 5250, Loss: 5.0125\n",
            "Batch 5300, Loss: 5.1885\n",
            "Batch 5350, Loss: 4.9123\n",
            "Batch 5400, Loss: 5.2929\n",
            "Fine-tuning checkpoint saved to /content/drive/My Drive/chatbot_data/checkpoints/finetune_epoch_1.pt\n",
            "✅ Fine-tuned model weights saved to /content/drive/My Drive/chatbot_data/model_output/chatbot_finetuned.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pvD18tZX96P",
        "outputId": "eb9f7cd7-e0b7-40e1-9dca-62ce70af6790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt: Hello, how are you today?\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  you: 0.0531\n",
            "  it: 0.0514\n",
            "  the: 0.0438\n",
            "  a: 0.0420\n",
            "  t: 0.0241\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  much: 0.2926\n",
            "  to: 0.0627\n",
            "  about: 0.0562\n",
            "  many: 0.0411\n",
            "  long: 0.0399\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  to: 0.0679\n",
            "  a: 0.0583\n",
            "  you: 0.0526\n",
            "  have: 0.0298\n",
            "  the: 0.0276\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  a: 0.0788\n",
            "  the: 0.0355\n",
            "  have: 0.0250\n",
            "  to: 0.0233\n",
            "  very: 0.0215\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  you: 0.0815\n",
            "  i: 0.0680\n",
            "  a: 0.0514\n",
            "  the: 0.0379\n",
            "  it: 0.0341\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  morning: 0.0463\n",
            "  way: 0.0303\n",
            "  afternoon: 0.0199\n",
            "  i: 0.0172\n",
            "  to: 0.0171\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  to: 0.0418\n",
            "  time: 0.0301\n",
            "  and: 0.0269\n",
            "  year: 0.0214\n",
            "  of: 0.0196\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  the: 0.0383\n",
            "  to: 0.0222\n",
            "  and: 0.0195\n",
            "  you: 0.0144\n",
            "  do: 0.0123\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  i: 0.2499\n",
            "  we: 0.0401\n",
            "  the: 0.0253\n",
            "  you: 0.0242\n",
            "  it: 0.0168\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  i: 0.1430\n",
            "  you: 0.0490\n",
            "  s: 0.0483\n",
            "  sorry: 0.0259\n",
            "  it: 0.0247\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  think: 0.1727\n",
            "  have: 0.0842\n",
            "  am: 0.0620\n",
            "  do: 0.0537\n",
            "  can: 0.0390\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  to: 0.1470\n",
            "  can: 0.0424\n",
            "  about: 0.0313\n",
            "  are: 0.0258\n",
            "  is: 0.0244\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  be: 0.1063\n",
            "  see: 0.0536\n",
            "  go: 0.0504\n",
            "  do: 0.0491\n",
            "  get: 0.0464\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  i: 0.0630\n",
            "  you: 0.0518\n",
            "  to: 0.0517\n",
            "  a: 0.0342\n",
            "  it: 0.0288\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  i: 0.0624\n",
            "  the: 0.0576\n",
            "  you: 0.0530\n",
            "  a: 0.0332\n",
            "  that: 0.0301\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  company: 0.0742\n",
            "  own: 0.0665\n",
            "  name: 0.0490\n",
            "  country: 0.0296\n",
            "  help: 0.0247\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  to: 0.0446\n",
            "  you: 0.0419\n",
            "  i: 0.0295\n",
            "  a: 0.0252\n",
            "  of: 0.0240\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  be: 0.1063\n",
            "  see: 0.0536\n",
            "  go: 0.0504\n",
            "  do: 0.0491\n",
            "  get: 0.0464\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  it: 0.0919\n",
            "  a: 0.0859\n",
            "  you: 0.0647\n",
            "  to: 0.0641\n",
            "  i: 0.0575\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  think: 0.1727\n",
            "  have: 0.0842\n",
            "  am: 0.0620\n",
            "  do: 0.0537\n",
            "  can: 0.0390\n",
            "Response: how well s afraid no feedback condition and step i talking to know home your offer to choose i don\n",
            "\n",
            "Prompt: What is the internet?\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  you: 0.0531\n",
            "  it: 0.0514\n",
            "  the: 0.0438\n",
            "  a: 0.0420\n",
            "  t: 0.0241\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  on: 0.0066\n",
            "  good: 0.0049\n",
            "  charge: 0.0035\n",
            "  need: 0.0034\n",
            "  there: 0.0031\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  to: 0.0911\n",
            "  a: 0.0447\n",
            "  the: 0.0329\n",
            "  i: 0.0307\n",
            "  you: 0.0279\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  think: 0.1727\n",
            "  have: 0.0842\n",
            "  am: 0.0620\n",
            "  do: 0.0537\n",
            "  can: 0.0390\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  i: 0.0765\n",
            "  you: 0.0673\n",
            "  it: 0.0469\n",
            "  that: 0.0404\n",
            "  a: 0.0398\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  i: 0.1555\n",
            "  you: 0.0464\n",
            "  we: 0.0312\n",
            "  it: 0.0199\n",
            "  is: 0.0192\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  a: 0.0881\n",
            "  to: 0.0731\n",
            "  the: 0.0393\n",
            "  in: 0.0271\n",
            "  that: 0.0225\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  to: 0.3444\n",
            "  that: 0.0715\n",
            "  a: 0.0602\n",
            "  it: 0.0512\n",
            "  the: 0.0279\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  world: 0.0329\n",
            "  internet: 0.0309\n",
            "  company: 0.0294\n",
            "  same: 0.0248\n",
            "  other: 0.0205\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  you: 0.0659\n",
            "  to: 0.0489\n",
            "  a: 0.0363\n",
            "  the: 0.0333\n",
            "  your: 0.0240\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  think: 0.1727\n",
            "  have: 0.0842\n",
            "  am: 0.0620\n",
            "  do: 0.0537\n",
            "  can: 0.0390\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  a: 0.0927\n",
            "  the: 0.0674\n",
            "  it: 0.0446\n",
            "  i: 0.0372\n",
            "  your: 0.0313\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  have: 0.1070\n",
            "  be: 0.0659\n",
            "  like: 0.0512\n",
            "  know: 0.0394\n",
            "  think: 0.0340\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  i: 0.0388\n",
            "  you: 0.0250\n",
            "  it: 0.0162\n",
            "  we: 0.0145\n",
            "  the: 0.0077\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  year: 0.0213\n",
            "  time: 0.0208\n",
            "  and: 0.0194\n",
            "  day: 0.0189\n",
            "  to: 0.0149\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  i: 0.0187\n",
            "  need: 0.0173\n",
            "  have: 0.0153\n",
            "  they: 0.0135\n",
            "  we: 0.0134\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  you: 0.0373\n",
            "  i: 0.0153\n",
            "  morning: 0.0131\n",
            "  a: 0.0108\n",
            "  problem: 0.0093\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  you: 0.0505\n",
            "  the: 0.0355\n",
            "  good: 0.0317\n",
            "  a: 0.0309\n",
            "  i: 0.0274\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  you: 0.1310\n",
            "  i: 0.0659\n",
            "  it: 0.0344\n",
            "  we: 0.0283\n",
            "  me: 0.0201\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  i: 0.1555\n",
            "  you: 0.0464\n",
            "  we: 0.0312\n",
            "  it: 0.0199\n",
            "  is: 0.0192\n",
            "Response: worker recognize i think that enjoyed like the benefit i forget t two practical years oval meat family that don\n",
            "\n",
            "Prompt: Tell me about yourself.\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  you: 0.0531\n",
            "  it: 0.0514\n",
            "  the: 0.0438\n",
            "  a: 0.0420\n",
            "  t: 0.0241\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  have: 0.3000\n",
            "  need: 0.0741\n",
            "  can: 0.0714\n",
            "  should: 0.0303\n",
            "  just: 0.0262\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  the: 0.1271\n",
            "  a: 0.0734\n",
            "  you: 0.0494\n",
            "  it: 0.0490\n",
            "  your: 0.0478\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  world: 0.0329\n",
            "  internet: 0.0309\n",
            "  company: 0.0294\n",
            "  same: 0.0248\n",
            "  other: 0.0205\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  you: 0.0776\n",
            "  to: 0.0346\n",
            "  me: 0.0263\n",
            "  about: 0.0249\n",
            "  is: 0.0222\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  you: 0.1069\n",
            "  i: 0.0511\n",
            "  to: 0.0374\n",
            "  it: 0.0361\n",
            "  of: 0.0339\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  you: 0.0652\n",
            "  i: 0.0458\n",
            "  it: 0.0248\n",
            "  of: 0.0237\n",
            "  to: 0.0225\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  i: 0.1555\n",
            "  you: 0.0464\n",
            "  we: 0.0312\n",
            "  it: 0.0199\n",
            "  is: 0.0192\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  good: 0.0048\n",
            "  this: 0.0036\n",
            "  we: 0.0033\n",
            "  on: 0.0033\n",
            "  i: 0.0025\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  is: 0.0462\n",
            "  i: 0.0430\n",
            "  can: 0.0393\n",
            "  to: 0.0291\n",
            "  are: 0.0244\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  be: 0.1537\n",
            "  you: 0.0886\n",
            "  like: 0.0613\n",
            "  help: 0.0362\n",
            "  have: 0.0327\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  can: 0.0299\n",
            "  have: 0.0222\n",
            "  like: 0.0179\n",
            "  want: 0.0148\n",
            "  will: 0.0139\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  you: 0.0581\n",
            "  a: 0.0438\n",
            "  i: 0.0401\n",
            "  the: 0.0351\n",
            "  to: 0.0273\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  is: 0.1547\n",
            "  was: 0.0968\n",
            "  will: 0.0285\n",
            "  can: 0.0253\n",
            "  are: 0.0249\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  to: 0.0997\n",
            "  and: 0.0617\n",
            "  of: 0.0462\n",
            "  i: 0.0459\n",
            "  in: 0.0248\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  i: 0.0180\n",
            "  so: 0.0174\n",
            "  s: 0.0168\n",
            "  go: 0.0140\n",
            "  have: 0.0126\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  i: 0.0925\n",
            "  you: 0.0837\n",
            "  that: 0.0430\n",
            "  it: 0.0404\n",
            "  a: 0.0355\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  you: 0.0510\n",
            "  a: 0.0423\n",
            "  to: 0.0302\n",
            "  i: 0.0207\n",
            "  is: 0.0204\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  the: 0.3601\n",
            "  a: 0.0861\n",
            "  my: 0.0611\n",
            "  your: 0.0444\n",
            "  our: 0.0209\n",
            "\n",
            "Top 5 predicted tokens:\n",
            "  company: 0.0394\n",
            "  you: 0.0092\n",
            "  time: 0.0080\n",
            "  morning: 0.0076\n",
            "  of: 0.0063\n",
            "Response: we see the beach mean crafts that its u would also feel it eight or convenient really in our have\n"
          ]
        }
      ],
      "source": [
        "def generate_response(prompt, max_length=20, temperature=1.0):\n",
        "    model.eval()\n",
        "    tokens = [token.lower() for token in Config.tokenizer(prompt) if token.lower() in word_to_idx]\n",
        "    if not tokens:\n",
        "        tokens = ['hello']\n",
        "    input_ids = [word_to_idx[token] for token in tokens]\n",
        "    input_ids = input_ids[-Config.seq_length+1:]\n",
        "    input_ids += [word_to_idx[Config.pad_token]] * (Config.seq_length-1 - len(input_ids))\n",
        "    input_tensor = torch.tensor([input_ids], dtype=torch.long).to(Config.device)\n",
        "\n",
        "    generated_ids = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            outputs = model(input_tensor)\n",
        "            logits = outputs[0, -1, :]\n",
        "            logits[word_to_idx[Config.unk_token]] = -float('inf')  # Suppress <unk>\n",
        "            logits = logits / temperature\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            # Print top 5 predicted tokens\n",
        "            top_probs, top_ids = torch.topk(probs, 5)\n",
        "            print(\"\\nTop 5 predicted tokens:\")\n",
        "            for prob, id in zip(top_probs.tolist(), top_ids.tolist()):\n",
        "                word = idx_to_word.get(str(id), '<not found>')\n",
        "                print(f\"  {word}: {prob:.4f}\")\n",
        "            next_token_id = torch.multinomial(probs, num_samples=1).item()\n",
        "            generated_ids.append(next_token_id)\n",
        "            input_ids = (input_ids[1:] + [next_token_id])[-Config.seq_length+1:]\n",
        "            input_tensor = torch.tensor([input_ids], dtype=torch.long).to(Config.device)\n",
        "            if next_token_id == word_to_idx[Config.pad_token]:\n",
        "                break\n",
        "\n",
        "    generated_words = [idx_to_word.get(str(id), Config.unk_token) for id in generated_ids]\n",
        "    return ' '.join(generated_words)\n",
        "\n",
        "# Test fine-tuned model\n",
        "test_prompts = [\n",
        "    \"Hello, how are you today?\",\n",
        "    \"What is the internet?\",\n",
        "    \"Tell me about yourself.\"\n",
        "]\n",
        "for prompt in test_prompts:\n",
        "    print(f\"\\nPrompt: {prompt}\")\n",
        "    response = generate_response(prompt, max_length=20, temperature=1.0)\n",
        "    print(f\"Response: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toDDSPQn_Bt6"
      },
      "source": [
        "## Load the model and generate responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GxyR_WY_E3C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a74786b-7b0f-47e7-ceb1-762388f727ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model loaded\n",
            "Vocab size from config: 48293\n",
            "Actual size of idx_to_word: 48293\n"
          ]
        }
      ],
      "source": [
        "# Define Transformer Model\n",
        "class MiniTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, n_heads=4, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, 128, d_model))\n",
        "        self.transformer = nn.TransformerDecoder(\n",
        "            nn.TransformerDecoderLayer(d_model, n_heads), num_layers\n",
        "        )\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        x = self.embed(input_ids) + self.pos_embed[:, :input_ids.size(1), :]\n",
        "        x = self.transformer(x, x)  # self-attention only\n",
        "        return self.fc(x)\n",
        "\n",
        "# Load config from /config\n",
        "with open(f'{target_directory}/config/config.json', 'r') as f:\n",
        "    config_dict = json.load(f)\n",
        "\n",
        "# Load vocab from /config\n",
        "with open(f'{target_directory}/config/word_to_idx.json', 'r') as f:\n",
        "    word_to_idx = json.load(f)\n",
        "with open(f'{target_directory}/config/idx_to_word.json', 'r') as f:\n",
        "    idx_to_word = json.load(f)\n",
        "\n",
        "# Reinitialize model & load fine-tuned weights\n",
        "model = MiniTransformer(config_dict['vocab_size']).to(config_dict['device'])\n",
        "model.load_state_dict(torch.load(f'{target_directory}/model_output/chatbot_finetuned.pt', map_location=config_dict['device']))\n",
        "model.eval()\n",
        "\n",
        "print(\"✅ Model loaded\")\n",
        "print(\"Vocab size from config:\", config_dict['vocab_size'])\n",
        "print(\"Actual size of idx_to_word:\", len(idx_to_word))\n",
        "\n",
        "# --- Set Config class dynamically ---\n",
        "class Config:\n",
        "    tokenizer = word_tokenize\n",
        "    seq_length = config_dict['seq_length']\n",
        "    batch_size = config_dict['batch_size']\n",
        "    learning_rate = config_dict['learning_rate']\n",
        "    device = config_dict['device']\n",
        "    vocab_size = config_dict['vocab_size']\n",
        "    pad_token = config_dict.get('pad_token', '<pad>')\n",
        "    unk_token = config_dict.get('unk_token', '<unk>')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text generation function\n",
        "def generate_response(model, prompt, max_length=50, top_p=0.9, temperature=0.7, repetition_penalty=2.0):\n",
        "    model.eval()\n",
        "    tokens = Config.tokenizer(prompt)\n",
        "    input_ids = [int(word_to_idx.get(token, word_to_idx[Config.unk_token])) for token in tokens]\n",
        "    input_ids = input_ids[:Config.seq_length - 1] + [word_to_idx[Config.pad_token]] * (Config.seq_length - 1 - len(input_ids))\n",
        "    input_ids = torch.tensor([input_ids], dtype=torch.long).to(Config.device)\n",
        "\n",
        "    generated_tokens = input_ids.clone()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            if input_ids.size(1) > Config.seq_length:\n",
        "                input_ids = input_ids[:, -Config.seq_length:]\n",
        "\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs[:, -1, :]\n",
        "\n",
        "            # Repetition penalty\n",
        "            for token in set(generated_tokens[0].tolist()):\n",
        "                logits[0, token] /= repetition_penalty\n",
        "\n",
        "            # Temperature scaling\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Top-p (nucleus) sampling\n",
        "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "            probs = torch.softmax(sorted_logits, dim=-1)\n",
        "            cumulative_probs = torch.cumsum(probs, dim=-1)\n",
        "            sorted_indices_to_keep = cumulative_probs <= top_p\n",
        "            if not sorted_indices_to_keep.any():\n",
        "                sorted_indices_to_keep[..., 0] = True\n",
        "            top_p_logits = sorted_logits[sorted_indices_to_keep]\n",
        "            top_p_indices = sorted_indices[sorted_indices_to_keep]\n",
        "            top_p_probs = torch.softmax(top_p_logits, dim=-1)\n",
        "\n",
        "            next_token_idx = torch.multinomial(top_p_probs, num_samples=1).item()\n",
        "            next_token = top_p_indices[next_token_idx].item()\n",
        "\n",
        "            input_ids = torch.cat([input_ids, torch.tensor([[next_token]]).to(Config.device)], dim=1)\n",
        "            generated_tokens = torch.cat([generated_tokens, torch.tensor([[next_token]]).to(Config.device)], dim=1)\n",
        "\n",
        "            if next_token == word_to_idx[Config.pad_token]:\n",
        "                break\n",
        "\n",
        "    return \" \".join([idx_to_word.get(str(idx), Config.unk_token) for idx in generated_tokens[0].tolist() if idx != word_to_idx[Config.pad_token]])\n",
        "\n",
        "# Chat loop\n",
        "print(\"\\n🟢 Chatbot ready! Type 'exit' to stop.\\n\")\n",
        "while True:\n",
        "    prompt = input(\"You: \")\n",
        "    if prompt.strip().lower() == \"exit\":\n",
        "        print(\"👋 Exiting chat.\")\n",
        "        break\n",
        "    response = generate_response(model, prompt)\n",
        "    print(f\"Bot: {response}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzG-TSrtBphk",
        "outputId": "10c618c6-e6f7-4241-fa84-4e9195c3fbb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🟢 Chatbot ready! Type 'exit' to stop.\n",
            "\n",
            "You: hi\n",
            "Bot: hi the customer <unk> please very good <unk> like a lot of work than your job with me about any movies you can say i don too much were doing profit to go in that afraid it is anything but we have no kitchen month <unk> sorry s be so many\n",
            "\n",
            "You: exit\n",
            "👋 Exiting chat.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}